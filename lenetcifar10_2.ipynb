{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lenetcifar10_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPSM6bnEcKI0/5HOXyOx3yq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chinmayee95/cnn-on-image-data/blob/master/lenetcifar10_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcvYtPGCLkeT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e5aef42-8ff5-4051-a8e3-067d38a5b06a"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        " \n",
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    if epoch > 100:\n",
        "        lrate = 0.0003\n",
        "    return lrate\n",
        " \n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        " \n",
        "#z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        " \n",
        "num_classes = 10\n",
        "y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "y_test = np_utils.to_categorical(y_test,num_classes)\n",
        " \n",
        "weight_decay = 1e-4\n",
        "lenet = Sequential()\n",
        "lenet.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "lenet.add(Activation('elu'))\n",
        "lenet.add(BatchNormalization())\n",
        "lenet.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "lenet.add(Activation('elu'))\n",
        "lenet.add(BatchNormalization())\n",
        "lenet.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# lenet.add(Dropout(0.5))\n",
        " \n",
        "lenet.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "lenet.add(Activation('elu'))\n",
        "lenet.add(BatchNormalization())\n",
        "lenet.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "lenet.add(Activation('elu'))\n",
        "lenet.add(BatchNormalization())\n",
        "lenet.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# lenet.add(Dropout(0.5))\n",
        " \n",
        "lenet.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "lenet.add(Activation('elu'))\n",
        "lenet.add(BatchNormalization())\n",
        "lenet.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "lenet.add(Activation('elu'))\n",
        "lenet.add(BatchNormalization())\n",
        "lenet.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# lenet.add(Dropout(0.5))\n",
        " \n",
        "lenet.add(Flatten())\n",
        "# lenet.add(Dense(512))\n",
        "lenet.add(Dense(1024, activation = keras.layers.LeakyReLU(alpha=0.1)))\n",
        "lenet.add(Dropout(0.5))\n",
        "\n",
        "lenet.add(Dense(512, activation = keras.layers.LeakyReLU(alpha=0.1)))\n",
        "lenet.add(Dropout(0.5))\n",
        "lenet.add(Dense(256, activation = keras.layers.LeakyReLU(alpha=0.1)))\n",
        "lenet.add(Dropout(0.5))\n",
        "\n",
        "lenet.add(Dense(128, activation = keras.layers.LeakyReLU(alpha=0.1)))\n",
        "lenet.add(Dropout(0.5))\n",
        "lenet.add(Dense(64, activation = keras.layers.LeakyReLU(alpha=0.1)))\n",
        "lenet.add(Dropout(0.5))\n",
        "lenet.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "lenet.summary()\n",
        " \n",
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "datagen.fit(x_train)\n",
        " \n",
        "#training\n",
        "batch_size = 64\n",
        "opt_adam = keras.optimizers.Adam(learning_rate=0.001, decay=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "lenet.compile(optimizer = opt_adam , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        " \n",
        "# opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
        "# lenet.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        "lenet.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
        "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "#save to disk\n",
        "lenet_json = lenet.to_json()\n",
        "with open('lenet.json', 'w') as json_file:\n",
        "    json_file.write(lenet_json)\n",
        "lenet.save_weights('lenet.h5') \n",
        " \n",
        "#testing\n",
        "scores = lenet.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
        "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 3,084,906\n",
            "Trainable params: 3,084,010\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "Epoch 1/125\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 2.1896 - accuracy: 0.2063 - val_loss: 1.7943 - val_accuracy: 0.3387\n",
            "Epoch 2/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 1.7208 - accuracy: 0.3580 - val_loss: 1.4855 - val_accuracy: 0.4568\n",
            "Epoch 3/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 1.4903 - accuracy: 0.4658 - val_loss: 1.2257 - val_accuracy: 0.5831\n",
            "Epoch 4/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 1.3160 - accuracy: 0.5604 - val_loss: 1.2240 - val_accuracy: 0.5874\n",
            "Epoch 5/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 1.1859 - accuracy: 0.6177 - val_loss: 1.0614 - val_accuracy: 0.6688\n",
            "Epoch 6/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 1.0821 - accuracy: 0.6649 - val_loss: 0.9108 - val_accuracy: 0.7128\n",
            "Epoch 7/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.0080 - accuracy: 0.6906 - val_loss: 0.8678 - val_accuracy: 0.7384\n",
            "Epoch 8/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.9557 - accuracy: 0.7148 - val_loss: 0.7529 - val_accuracy: 0.7764\n",
            "Epoch 9/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.8974 - accuracy: 0.7422 - val_loss: 0.7963 - val_accuracy: 0.7679\n",
            "Epoch 10/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.8606 - accuracy: 0.7542 - val_loss: 0.8053 - val_accuracy: 0.7604\n",
            "Epoch 11/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.8133 - accuracy: 0.7720 - val_loss: 0.7219 - val_accuracy: 0.7900\n",
            "Epoch 12/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.7848 - accuracy: 0.7834 - val_loss: 0.6466 - val_accuracy: 0.8193\n",
            "Epoch 13/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7478 - accuracy: 0.7939 - val_loss: 0.6766 - val_accuracy: 0.8029\n",
            "Epoch 14/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.7235 - accuracy: 0.8049 - val_loss: 0.6686 - val_accuracy: 0.8139\n",
            "Epoch 15/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6997 - accuracy: 0.8118 - val_loss: 0.6586 - val_accuracy: 0.8218\n",
            "Epoch 16/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6848 - accuracy: 0.8177 - val_loss: 0.6475 - val_accuracy: 0.8274\n",
            "Epoch 17/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6622 - accuracy: 0.8237 - val_loss: 0.5949 - val_accuracy: 0.8427\n",
            "Epoch 18/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.6457 - accuracy: 0.8297 - val_loss: 0.6794 - val_accuracy: 0.8155\n",
            "Epoch 19/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6363 - accuracy: 0.8340 - val_loss: 0.6593 - val_accuracy: 0.8128\n",
            "Epoch 20/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6168 - accuracy: 0.8375 - val_loss: 0.6172 - val_accuracy: 0.8381\n",
            "Epoch 21/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.6009 - accuracy: 0.8443 - val_loss: 0.6065 - val_accuracy: 0.8363\n",
            "Epoch 22/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5839 - accuracy: 0.8479 - val_loss: 0.5640 - val_accuracy: 0.8522\n",
            "Epoch 23/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5719 - accuracy: 0.8524 - val_loss: 0.5826 - val_accuracy: 0.8450\n",
            "Epoch 24/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5625 - accuracy: 0.8562 - val_loss: 0.5779 - val_accuracy: 0.8464\n",
            "Epoch 25/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5543 - accuracy: 0.8603 - val_loss: 0.5692 - val_accuracy: 0.8473\n",
            "Epoch 26/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5384 - accuracy: 0.8657 - val_loss: 0.5938 - val_accuracy: 0.8430\n",
            "Epoch 27/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5296 - accuracy: 0.8659 - val_loss: 0.5970 - val_accuracy: 0.8458\n",
            "Epoch 28/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5254 - accuracy: 0.8675 - val_loss: 0.5843 - val_accuracy: 0.8533\n",
            "Epoch 29/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5123 - accuracy: 0.8726 - val_loss: 0.5251 - val_accuracy: 0.8651\n",
            "Epoch 30/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5057 - accuracy: 0.8733 - val_loss: 0.5439 - val_accuracy: 0.8557\n",
            "Epoch 31/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4919 - accuracy: 0.8763 - val_loss: 0.5398 - val_accuracy: 0.8622\n",
            "Epoch 32/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4856 - accuracy: 0.8807 - val_loss: 0.5419 - val_accuracy: 0.8597\n",
            "Epoch 33/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4757 - accuracy: 0.8825 - val_loss: 0.5543 - val_accuracy: 0.8596\n",
            "Epoch 34/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4708 - accuracy: 0.8844 - val_loss: 0.5227 - val_accuracy: 0.8716\n",
            "Epoch 35/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4640 - accuracy: 0.8857 - val_loss: 0.5006 - val_accuracy: 0.8750\n",
            "Epoch 36/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4577 - accuracy: 0.8897 - val_loss: 0.5347 - val_accuracy: 0.8657\n",
            "Epoch 37/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4517 - accuracy: 0.8896 - val_loss: 0.5553 - val_accuracy: 0.8620\n",
            "Epoch 38/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4432 - accuracy: 0.8925 - val_loss: 0.4977 - val_accuracy: 0.8741\n",
            "Epoch 39/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4365 - accuracy: 0.8956 - val_loss: 0.5052 - val_accuracy: 0.8750\n",
            "Epoch 40/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4322 - accuracy: 0.8953 - val_loss: 0.5261 - val_accuracy: 0.8708\n",
            "Epoch 41/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4313 - accuracy: 0.8988 - val_loss: 0.5408 - val_accuracy: 0.8648\n",
            "Epoch 42/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4196 - accuracy: 0.9004 - val_loss: 0.5256 - val_accuracy: 0.8656\n",
            "Epoch 43/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4187 - accuracy: 0.8993 - val_loss: 0.5367 - val_accuracy: 0.8694\n",
            "Epoch 44/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4105 - accuracy: 0.9014 - val_loss: 0.5019 - val_accuracy: 0.8763\n",
            "Epoch 45/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4078 - accuracy: 0.9031 - val_loss: 0.5587 - val_accuracy: 0.8620\n",
            "Epoch 46/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4044 - accuracy: 0.9050 - val_loss: 0.4999 - val_accuracy: 0.8790\n",
            "Epoch 47/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4002 - accuracy: 0.9063 - val_loss: 0.4870 - val_accuracy: 0.8781\n",
            "Epoch 48/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3925 - accuracy: 0.9092 - val_loss: 0.5123 - val_accuracy: 0.8748\n",
            "Epoch 49/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3840 - accuracy: 0.9105 - val_loss: 0.5113 - val_accuracy: 0.8798\n",
            "Epoch 50/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3803 - accuracy: 0.9110 - val_loss: 0.5669 - val_accuracy: 0.8649\n",
            "Epoch 51/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3837 - accuracy: 0.9099 - val_loss: 0.5483 - val_accuracy: 0.8679\n",
            "Epoch 52/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3740 - accuracy: 0.9128 - val_loss: 0.5047 - val_accuracy: 0.8798\n",
            "Epoch 53/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3726 - accuracy: 0.9141 - val_loss: 0.5150 - val_accuracy: 0.8776\n",
            "Epoch 54/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3666 - accuracy: 0.9149 - val_loss: 0.5060 - val_accuracy: 0.8811\n",
            "Epoch 55/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3696 - accuracy: 0.9153 - val_loss: 0.5678 - val_accuracy: 0.8707\n",
            "Epoch 56/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3641 - accuracy: 0.9175 - val_loss: 0.5259 - val_accuracy: 0.8819\n",
            "Epoch 57/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3611 - accuracy: 0.9171 - val_loss: 0.5182 - val_accuracy: 0.8781\n",
            "Epoch 58/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3553 - accuracy: 0.9183 - val_loss: 0.4934 - val_accuracy: 0.8838\n",
            "Epoch 59/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3510 - accuracy: 0.9195 - val_loss: 0.5251 - val_accuracy: 0.8817\n",
            "Epoch 60/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3478 - accuracy: 0.9205 - val_loss: 0.5509 - val_accuracy: 0.8738\n",
            "Epoch 61/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3431 - accuracy: 0.9229 - val_loss: 0.5141 - val_accuracy: 0.8820\n",
            "Epoch 62/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3428 - accuracy: 0.9229 - val_loss: 0.5120 - val_accuracy: 0.8828\n",
            "Epoch 63/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3388 - accuracy: 0.9236 - val_loss: 0.4886 - val_accuracy: 0.8849\n",
            "Epoch 64/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3292 - accuracy: 0.9265 - val_loss: 0.5182 - val_accuracy: 0.8845\n",
            "Epoch 65/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3340 - accuracy: 0.9247 - val_loss: 0.5114 - val_accuracy: 0.8842\n",
            "Epoch 66/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3309 - accuracy: 0.9259 - val_loss: 0.5037 - val_accuracy: 0.8811\n",
            "Epoch 67/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3267 - accuracy: 0.9297 - val_loss: 0.5141 - val_accuracy: 0.8840\n",
            "Epoch 68/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3288 - accuracy: 0.9257 - val_loss: 0.5181 - val_accuracy: 0.8808\n",
            "Epoch 69/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3235 - accuracy: 0.9282 - val_loss: 0.4965 - val_accuracy: 0.8853\n",
            "Epoch 70/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3255 - accuracy: 0.9286 - val_loss: 0.5170 - val_accuracy: 0.8807\n",
            "Epoch 71/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.3151 - accuracy: 0.9304 - val_loss: 0.4866 - val_accuracy: 0.8871\n",
            "Epoch 72/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.3130 - accuracy: 0.9315 - val_loss: 0.5152 - val_accuracy: 0.8817\n",
            "Epoch 73/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3142 - accuracy: 0.9312 - val_loss: 0.5063 - val_accuracy: 0.8867\n",
            "Epoch 74/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3151 - accuracy: 0.9315 - val_loss: 0.5144 - val_accuracy: 0.8837\n",
            "Epoch 75/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3088 - accuracy: 0.9321 - val_loss: 0.5012 - val_accuracy: 0.8894\n",
            "Epoch 76/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3045 - accuracy: 0.9326 - val_loss: 0.4979 - val_accuracy: 0.8874\n",
            "Epoch 77/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2897 - accuracy: 0.9398 - val_loss: 0.5024 - val_accuracy: 0.8881\n",
            "Epoch 78/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2804 - accuracy: 0.9418 - val_loss: 0.5088 - val_accuracy: 0.8863\n",
            "Epoch 79/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2735 - accuracy: 0.9423 - val_loss: 0.4868 - val_accuracy: 0.8972\n",
            "Epoch 80/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2703 - accuracy: 0.9434 - val_loss: 0.5158 - val_accuracy: 0.8900\n",
            "Epoch 81/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.2686 - accuracy: 0.9442 - val_loss: 0.5155 - val_accuracy: 0.8932\n",
            "Epoch 82/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2714 - accuracy: 0.9434 - val_loss: 0.4910 - val_accuracy: 0.8919\n",
            "Epoch 83/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2672 - accuracy: 0.9460 - val_loss: 0.4909 - val_accuracy: 0.8945\n",
            "Epoch 84/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.2656 - accuracy: 0.9439 - val_loss: 0.5063 - val_accuracy: 0.8895\n",
            "Epoch 85/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2639 - accuracy: 0.9456 - val_loss: 0.4815 - val_accuracy: 0.8977\n",
            "Epoch 86/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.2636 - accuracy: 0.9457 - val_loss: 0.4984 - val_accuracy: 0.8939\n",
            "Epoch 87/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2602 - accuracy: 0.9462 - val_loss: 0.5047 - val_accuracy: 0.8883\n",
            "Epoch 88/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.2580 - accuracy: 0.9461 - val_loss: 0.5011 - val_accuracy: 0.8963\n",
            "Epoch 89/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2551 - accuracy: 0.9483 - val_loss: 0.4992 - val_accuracy: 0.8942\n",
            "Epoch 90/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2600 - accuracy: 0.9469 - val_loss: 0.4884 - val_accuracy: 0.8988\n",
            "Epoch 91/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2534 - accuracy: 0.9476 - val_loss: 0.4931 - val_accuracy: 0.8944\n",
            "Epoch 92/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2542 - accuracy: 0.9487 - val_loss: 0.5061 - val_accuracy: 0.8916\n",
            "Epoch 93/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2535 - accuracy: 0.9484 - val_loss: 0.5020 - val_accuracy: 0.8935\n",
            "Epoch 94/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2476 - accuracy: 0.9492 - val_loss: 0.5233 - val_accuracy: 0.8880\n",
            "Epoch 95/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2497 - accuracy: 0.9482 - val_loss: 0.5059 - val_accuracy: 0.8944\n",
            "Epoch 96/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.2540 - accuracy: 0.9486 - val_loss: 0.4794 - val_accuracy: 0.8989\n",
            "Epoch 97/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2478 - accuracy: 0.9507 - val_loss: 0.4919 - val_accuracy: 0.8959\n",
            "Epoch 98/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2479 - accuracy: 0.9508 - val_loss: 0.4933 - val_accuracy: 0.8940\n",
            "Epoch 99/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.2512 - accuracy: 0.9496 - val_loss: 0.5040 - val_accuracy: 0.8898\n",
            "Epoch 100/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.2459 - accuracy: 0.9514 - val_loss: 0.4814 - val_accuracy: 0.8937\n",
            "Epoch 101/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2434 - accuracy: 0.9506 - val_loss: 0.4965 - val_accuracy: 0.8959\n",
            "Epoch 102/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2402 - accuracy: 0.9523 - val_loss: 0.4881 - val_accuracy: 0.8993\n",
            "Epoch 103/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2372 - accuracy: 0.9533 - val_loss: 0.4866 - val_accuracy: 0.8968\n",
            "Epoch 104/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2303 - accuracy: 0.9540 - val_loss: 0.4967 - val_accuracy: 0.8971\n",
            "Epoch 105/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2324 - accuracy: 0.9544 - val_loss: 0.4901 - val_accuracy: 0.8983\n",
            "Epoch 106/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2340 - accuracy: 0.9535 - val_loss: 0.4939 - val_accuracy: 0.8966\n",
            "Epoch 107/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2316 - accuracy: 0.9546 - val_loss: 0.5003 - val_accuracy: 0.8976\n",
            "Epoch 108/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.2326 - accuracy: 0.9543 - val_loss: 0.5000 - val_accuracy: 0.8977\n",
            "Epoch 109/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2323 - accuracy: 0.9546 - val_loss: 0.4938 - val_accuracy: 0.8965\n",
            "Epoch 110/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2329 - accuracy: 0.9550 - val_loss: 0.4905 - val_accuracy: 0.8982\n",
            "Epoch 111/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2235 - accuracy: 0.9552 - val_loss: 0.5064 - val_accuracy: 0.8976\n",
            "Epoch 112/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.2262 - accuracy: 0.9550 - val_loss: 0.4997 - val_accuracy: 0.8966\n",
            "Epoch 113/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2245 - accuracy: 0.9566 - val_loss: 0.5094 - val_accuracy: 0.8959\n",
            "Epoch 114/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2231 - accuracy: 0.9568 - val_loss: 0.5064 - val_accuracy: 0.8962\n",
            "Epoch 115/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.2257 - accuracy: 0.9555 - val_loss: 0.5019 - val_accuracy: 0.8967\n",
            "Epoch 116/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2251 - accuracy: 0.9558 - val_loss: 0.5046 - val_accuracy: 0.8972\n",
            "Epoch 117/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2251 - accuracy: 0.9554 - val_loss: 0.5057 - val_accuracy: 0.8973\n",
            "Epoch 118/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2267 - accuracy: 0.9555 - val_loss: 0.5173 - val_accuracy: 0.8957\n",
            "Epoch 119/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2211 - accuracy: 0.9570 - val_loss: 0.5147 - val_accuracy: 0.8974\n",
            "Epoch 120/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2181 - accuracy: 0.9578 - val_loss: 0.5095 - val_accuracy: 0.8993\n",
            "Epoch 121/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2223 - accuracy: 0.9560 - val_loss: 0.5055 - val_accuracy: 0.8992\n",
            "Epoch 122/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2225 - accuracy: 0.9560 - val_loss: 0.5081 - val_accuracy: 0.8977\n",
            "Epoch 123/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2202 - accuracy: 0.9574 - val_loss: 0.5172 - val_accuracy: 0.8978\n",
            "Epoch 124/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2215 - accuracy: 0.9575 - val_loss: 0.5024 - val_accuracy: 0.8988\n",
            "Epoch 125/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.2177 - accuracy: 0.9587 - val_loss: 0.5035 - val_accuracy: 0.8995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0c53ef45df71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                    \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m,\u001b[0m                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLearningRateScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m#save to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1278\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not JSON Serializable:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_json_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_updated_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         model_config = {\n\u001b[1;32m   1246\u001b[0m             \u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m             layer_configs.append({\n\u001b[1;32m    278\u001b[0m                 \u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0;34m'config'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m             })\n\u001b[1;32m    281\u001b[0m         config = {\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    923\u001b[0m         config = {\n\u001b[1;32m    924\u001b[0m             \u001b[0;34m'units'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;34m'activation'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m             \u001b[0;34m'use_bias'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;34m'kernel_initializer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/activations.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(activation)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LeakyReLU' object has no attribute '__name__'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5wlgIEEMLHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = le_net.evaluate(data_test, label_test, verbose=1)\n",
        "\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrPaAgSCpaD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lenet_summary = lenet_fit.history\n",
        "\n",
        "loss_values = lenet_summary['loss']\n",
        "val_loss_values = lenet_summary['val_loss']\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "line1 = plt.plot(epochs, val_loss_values, label='Test Loss')\n",
        "line2 = plt.plot(epochs, loss_values, label='Training Loss')\n",
        "plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n",
        "plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n",
        "plt.xlabel('Epochs') \n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr4fi4i7pd3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting our accuracy charts\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lenet_summary = lenet_fit.history\n",
        "\n",
        "acc_values = lenet_summary['accuracy']\n",
        "val_acc_values = lenet_summary['val_accuracy']\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "line1 = plt.plot(epochs, val_acc_values, label='Test Accuracy')\n",
        "line2 = plt.plot(epochs, acc_values, label='Training Accuracy')\n",
        "plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n",
        "plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n",
        "plt.xlabel('Epochs') \n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}